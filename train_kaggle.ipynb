{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a184b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers>=4.30.0 mediapipe peft>=0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to /kaggle/working\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': '/kaggle/input/wlalsl-with-landmarks/data/wlasl',\n",
    "    'landmarks_dir': '/kaggle/input/wlalsl-with-landmarks/data/wlasl/landmarks',\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    'subset': 'nslt_1000.json',\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 50,\n",
    "    'num_frames': 16,\n",
    "    'hidden_dim': 256,\n",
    "    'learning_rate': 1e-4,\n",
    "    'fusion_type': 'concat',\n",
    "    'device': 'cuda',\n",
    "    # Augmentation settings\n",
    "    'augment': True,\n",
    "    'augment_strength': 'medium',  # 'light', 'medium', 'strong'\n",
    "    # Regularization settings  \n",
    "    'dropout': 0.5,\n",
    "    'weight_decay': 0.05,\n",
    "    'label_smoothing': 0.1,\n",
    "    # LoRA settings (parameter-efficient fine-tuning)\n",
    "    'use_lora': True,            # Enable LoRA for VideoMAE\n",
    "    'lora_rank': 8,              # Low-rank dimension\n",
    "    'lora_alpha': 16,            # Scaling factor\n",
    "    'lora_dropout': 0.1,         # Dropout for LoRA layers\n",
    "    'freeze_videomae': False,    # Must be False when using LoRA\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MediaPipe models\n",
    "import urllib.request\n",
    "\n",
    "def download_mediapipe_models():\n",
    "    \"\"\"Download MediaPipe task models to working directory.\"\"\"\n",
    "    models = {\n",
    "        'hand_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',\n",
    "        'pose_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task'\n",
    "    }\n",
    "    \n",
    "    for filename, url in models.items():\n",
    "        if not Path(filename).exists():\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            print(f\"✓ Downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"✓ {filename} already exists\")\n",
    "\n",
    "download_mediapipe_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy source files from input dataset if using custom dataset structure\n",
    "# If your dataset includes the python files, copy them:\n",
    "source_files = ['hybrid_asl_model.py', 'train_hybrid_asl.py']\n",
    "for f in source_files:\n",
    "    src = Path(f'/kaggle/input/wlasl/{f}')\n",
    "    dst = Path(f'/kaggle/working/{f}')\n",
    "    if src.exists() and not dst.exists():\n",
    "        import shutil\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"Copied {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1beb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# DATA AUGMENTATION CLASSES (inline for Kaggle notebook portability)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class VideoAugmentation:\n",
    "    \"\"\"Video frame augmentations for ASL recognition.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        enabled: bool = True,\n",
    "        brightness_range: Tuple[float, float] = (0.7, 1.3),\n",
    "        contrast_range: Tuple[float, float] = (0.7, 1.3),\n",
    "        saturation_range: Tuple[float, float] = (0.8, 1.2),\n",
    "        rotation_degrees: float = 15.0,\n",
    "        scale_range: Tuple[float, float] = (0.85, 1.0),\n",
    "        gaussian_noise_std: float = 0.02,\n",
    "    ):\n",
    "        self.enabled = enabled\n",
    "        self.brightness_range = brightness_range\n",
    "        self.contrast_range = contrast_range\n",
    "        self.saturation_range = saturation_range\n",
    "        self.rotation_degrees = rotation_degrees\n",
    "        self.scale_range = scale_range\n",
    "        self.gaussian_noise_std = gaussian_noise_std\n",
    "    \n",
    "    def __call__(self, frames: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        if not self.enabled or len(frames) == 0:\n",
    "            return frames\n",
    "        \n",
    "        brightness = random.uniform(*self.brightness_range)\n",
    "        contrast = random.uniform(*self.contrast_range)\n",
    "        angle = random.uniform(-self.rotation_degrees, self.rotation_degrees)\n",
    "        scale = random.uniform(*self.scale_range)\n",
    "        \n",
    "        augmented = []\n",
    "        for frame in frames:\n",
    "            aug_frame = frame.copy().astype(np.float32)\n",
    "            \n",
    "            # Brightness & contrast\n",
    "            aug_frame = aug_frame * brightness\n",
    "            mean = aug_frame.mean()\n",
    "            aug_frame = (aug_frame - mean) * contrast + mean\n",
    "            \n",
    "            # Rotation & scale\n",
    "            if abs(angle) > 0.5 or scale != 1.0:\n",
    "                h, w = aug_frame.shape[:2]\n",
    "                center = (w // 2, h // 2)\n",
    "                M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "                aug_frame = cv2.warpAffine(aug_frame, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "            \n",
    "            # Gaussian noise\n",
    "            if self.gaussian_noise_std > 0:\n",
    "                noise = np.random.normal(0, self.gaussian_noise_std * 255, aug_frame.shape)\n",
    "                aug_frame = aug_frame + noise\n",
    "            \n",
    "            aug_frame = np.clip(aug_frame, 0, 255).astype(np.uint8)\n",
    "            augmented.append(aug_frame)\n",
    "        \n",
    "        return augmented\n",
    "\n",
    "\n",
    "class LandmarkAugmentation:\n",
    "    \"\"\"Landmark augmentations for MediaPipe hand/pose landmarks.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        enabled: bool = True,\n",
    "        noise_std: float = 0.02,\n",
    "        temporal_shift_range: int = 2,\n",
    "        scale_range: Tuple[float, float] = (0.9, 1.1),\n",
    "        dropout_prob: float = 0.05,\n",
    "        translation_range: float = 0.05,\n",
    "    ):\n",
    "        self.enabled = enabled\n",
    "        self.noise_std = noise_std\n",
    "        self.temporal_shift_range = temporal_shift_range\n",
    "        self.scale_range = scale_range\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.translation_range = translation_range\n",
    "    \n",
    "    def __call__(self, landmarks: np.ndarray) -> np.ndarray:\n",
    "        if not self.enabled:\n",
    "            return landmarks\n",
    "        \n",
    "        landmarks = landmarks.copy().astype(np.float32)\n",
    "        \n",
    "        # Gaussian noise\n",
    "        if self.noise_std > 0:\n",
    "            noise = np.random.normal(0, self.noise_std, landmarks.shape)\n",
    "            landmarks = landmarks + noise\n",
    "        \n",
    "        # Temporal shift\n",
    "        if self.temporal_shift_range > 0:\n",
    "            shift = random.randint(-self.temporal_shift_range, self.temporal_shift_range)\n",
    "            if shift != 0:\n",
    "                landmarks = np.roll(landmarks, shift, axis=0)\n",
    "        \n",
    "        # Scale\n",
    "        if self.scale_range != (1.0, 1.0):\n",
    "            scale = random.uniform(*self.scale_range)\n",
    "            landmarks = (landmarks - 0.5) * scale + 0.5\n",
    "        \n",
    "        # Translation\n",
    "        if self.translation_range > 0:\n",
    "            tx = random.uniform(-self.translation_range, self.translation_range)\n",
    "            ty = random.uniform(-self.translation_range, self.translation_range)\n",
    "            for i in range(0, landmarks.shape[1], 3):\n",
    "                landmarks[:, i] += tx\n",
    "            for i in range(1, landmarks.shape[1], 3):\n",
    "                landmarks[:, i] += ty\n",
    "        \n",
    "        # Dropout\n",
    "        if self.dropout_prob > 0:\n",
    "            mask = np.random.random(landmarks.shape[0]) > self.dropout_prob\n",
    "            landmarks[~mask] = 0.0\n",
    "        \n",
    "        return landmarks.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_augmentations(enabled: bool = True, strength: str = 'medium'):\n",
    "    \"\"\"Factory function to create augmentation objects.\"\"\"\n",
    "    if not enabled:\n",
    "        return VideoAugmentation(enabled=False), LandmarkAugmentation(enabled=False)\n",
    "    \n",
    "    configs = {\n",
    "        'light': {\n",
    "            'video': {'brightness_range': (0.85, 1.15), 'contrast_range': (0.85, 1.15), \n",
    "                      'rotation_degrees': 8.0, 'scale_range': (0.92, 1.0), 'gaussian_noise_std': 0.01},\n",
    "            'landmark': {'noise_std': 0.01, 'temporal_shift_range': 1, \n",
    "                         'scale_range': (0.95, 1.05), 'translation_range': 0.02}\n",
    "        },\n",
    "        'medium': {\n",
    "            'video': {'brightness_range': (0.7, 1.3), 'contrast_range': (0.7, 1.3),\n",
    "                      'rotation_degrees': 15.0, 'scale_range': (0.85, 1.0), 'gaussian_noise_std': 0.02},\n",
    "            'landmark': {'noise_std': 0.02, 'temporal_shift_range': 2,\n",
    "                         'scale_range': (0.9, 1.1), 'translation_range': 0.05}\n",
    "        },\n",
    "        'strong': {\n",
    "            'video': {'brightness_range': (0.6, 1.4), 'contrast_range': (0.6, 1.4),\n",
    "                      'rotation_degrees': 20.0, 'scale_range': (0.75, 1.0), 'gaussian_noise_std': 0.03},\n",
    "            'landmark': {'noise_std': 0.03, 'temporal_shift_range': 3,\n",
    "                         'scale_range': (0.85, 1.15), 'translation_range': 0.08, 'dropout_prob': 0.1}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    cfg = configs.get(strength, configs['medium'])\n",
    "    return VideoAugmentation(enabled=True, **cfg['video']), LandmarkAugmentation(enabled=True, **cfg['landmark'])\n",
    "\n",
    "# Create augmentation objects based on config\n",
    "video_augment, landmark_augment = get_augmentations(\n",
    "    enabled=CONFIG['augment'], \n",
    "    strength=CONFIG['augment_strength']\n",
    ")\n",
    "print(f\"✓ Augmentation: {'ENABLED' if CONFIG['augment'] else 'disabled'} (strength={CONFIG['augment_strength']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d483c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import VideoMAEModel, VideoMAEImageProcessor\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Try to import PEFT for LoRA\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    PEFT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PEFT_AVAILABLE = False\n",
    "    print(\"Warning: PEFT not available. LoRA will be disabled.\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"PEFT available: {PEFT_AVAILABLE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# MODEL DEFINITIONS (inline for Kaggle notebook portability)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def get_lora_config(rank=8, alpha=16, dropout=0.1):\n",
    "    \"\"\"Create LoRA configuration for VideoMAE.\"\"\"\n",
    "    if not PEFT_AVAILABLE:\n",
    "        return None\n",
    "    return LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "        target_modules=[\"query\", \"value\", \"key\", \"dense\"],\n",
    "        bias=\"none\",\n",
    "        modules_to_save=None,\n",
    "    )\n",
    "\n",
    "def apply_lora_to_videomae(model, lora_config):\n",
    "    \"\"\"Apply LoRA to a VideoMAE model.\"\"\"\n",
    "    if not PEFT_AVAILABLE or lora_config is None:\n",
    "        return model\n",
    "    return get_peft_model(model, lora_config)\n",
    "\n",
    "class LandmarkEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder for landmark sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=162, hidden_dim=256, num_heads=4, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 64, hidden_dim) * 0.02)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VideoMAEEncoder(nn.Module):\n",
    "    \"\"\"VideoMAE encoder for visual features with optional LoRA.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='MCG-NJU/videomae-base', hidden_dim=256, freeze_backbone=False, dropout=0.3,\n",
    "                 use_lora=False, lora_rank=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.videomae = VideoMAEModel.from_pretrained(model_name)\n",
    "        self.use_lora = use_lora and PEFT_AVAILABLE\n",
    "        \n",
    "        # Apply LoRA if enabled (mutually exclusive with freeze_backbone)\n",
    "        if self.use_lora:\n",
    "            lora_config = get_lora_config(rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)\n",
    "            self.videomae = apply_lora_to_videomae(self.videomae, lora_config)\n",
    "            print(f\"  LoRA applied to VideoMAE (rank={lora_rank}, alpha={lora_alpha})\")\n",
    "        elif freeze_backbone:\n",
    "            for param in self.videomae.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        videomae_hidden = self.videomae.config.hidden_size\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(videomae_hidden, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.videomae(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        features = self.projection(pooled)\n",
    "        return features\n",
    "\n",
    "\n",
    "class HybridASLModel(nn.Module):\n",
    "    \"\"\"Hybrid model combining VideoMAE and MediaPipe landmarks.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, videomae_model='MCG-NJU/videomae-base', hidden_dim=256,\n",
    "                 freeze_videomae=False, dropout=0.3, fusion_type='concat',\n",
    "                 use_lora=False, lora_rank=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.visual_encoder = VideoMAEEncoder(\n",
    "            model_name=videomae_model,\n",
    "            hidden_dim=hidden_dim,\n",
    "            freeze_backbone=freeze_videomae,\n",
    "            dropout=dropout,\n",
    "            use_lora=use_lora,\n",
    "            lora_rank=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout\n",
    "        )\n",
    "        \n",
    "        self.landmark_encoder = LandmarkEncoder(\n",
    "            input_dim=162,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        if fusion_type == 'concat':\n",
    "            fusion_input_dim = hidden_dim * 2\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(fusion_input_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        elif fusion_type == 'attention':\n",
    "            self.fusion_attention = nn.MultiheadAttention(\n",
    "                embed_dim=hidden_dim,\n",
    "                num_heads=4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        elif fusion_type == 'gated':\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, pixel_values, landmarks):\n",
    "        visual_features = self.visual_encoder(pixel_values)\n",
    "        landmark_features = self.landmark_encoder(landmarks)\n",
    "        \n",
    "        if self.fusion_type == 'concat':\n",
    "            fused = torch.cat([visual_features, landmark_features], dim=-1)\n",
    "            fused = self.fusion(fused)\n",
    "        elif self.fusion_type == 'attention':\n",
    "            v_expanded = visual_features.unsqueeze(1)\n",
    "            l_expanded = landmark_features.unsqueeze(1)\n",
    "            attended_v, _ = self.fusion_attention(v_expanded, l_expanded, l_expanded)\n",
    "            attended_l, _ = self.fusion_attention(l_expanded, v_expanded, v_expanded)\n",
    "            fused = torch.cat([attended_v.squeeze(1), attended_l.squeeze(1)], dim=-1)\n",
    "            fused = self.fusion(fused)\n",
    "        elif self.fusion_type == 'gated':\n",
    "            combined = torch.cat([visual_features, landmark_features], dim=-1)\n",
    "            gate = self.gate(combined)\n",
    "            fused = gate * visual_features + (1 - gate) * landmark_features\n",
    "            fused = self.fusion(fused)\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, pixel_values, landmarks):\n",
    "        logits = self.forward(pixel_values, landmarks)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        confidence, predictions = probs.max(dim=-1)\n",
    "        return predictions, confidence, probs\n",
    "\n",
    "print(\"✓ Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ccb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# DATASET CLASS WITH LANDMARK CACHING, FRAME TRIMMING, AND AUGMENTATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class HybridASLDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with pre-extracted landmark support, frame trimming, and augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, video_paths, labels, videomae_processor, num_frames=16, \n",
    "                 image_size=224, landmarks_dir=None, frame_info=None,\n",
    "                 video_augment=None, landmark_augment=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.videomae_processor = videomae_processor\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.landmarks_dir = Path(landmarks_dir) if landmarks_dir else None\n",
    "        self.frame_info = frame_info  # (start_frame, end_frame) for each video\n",
    "        self.video_augment = video_augment\n",
    "        self.landmark_augment = landmark_augment\n",
    "        \n",
    "        # Build landmarks cache lookup\n",
    "        self.cached_landmarks = {}\n",
    "        if self.landmarks_dir:\n",
    "            for vp in video_paths:\n",
    "                video_id = Path(vp).stem\n",
    "                landmark_path = self.landmarks_dir / f\"{video_id}_landmarks.npy\"\n",
    "                if landmark_path.exists():\n",
    "                    self.cached_landmarks[video_id] = str(landmark_path)\n",
    "            print(f\"  Found {len(self.cached_landmarks)}/{len(video_paths)} pre-extracted landmarks\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def load_video_frames(self, video_path, start_frame=None, end_frame=None):\n",
    "        \"\"\"Load video frames with optional frame trimming.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Apply frame trimming if specified (convert 1-indexed to 0-indexed)\n",
    "        if start_frame is not None and end_frame is not None:\n",
    "            frame_start = max(0, start_frame - 1)\n",
    "            frame_end = min(total_frames - 1, end_frame - 1)\n",
    "            segment_length = frame_end - frame_start + 1\n",
    "        else:\n",
    "            frame_start = 0\n",
    "            frame_end = total_frames - 1\n",
    "            segment_length = total_frames\n",
    "        \n",
    "        # Sample frames uniformly within the trimmed segment\n",
    "        if segment_length <= self.num_frames:\n",
    "            indices = [frame_start + i for i in range(segment_length)]\n",
    "        else:\n",
    "            indices = np.linspace(frame_start, frame_end, self.num_frames, dtype=int).tolist()\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1] if frames else np.zeros((self.image_size, self.image_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        return frames[:self.num_frames]\n",
    "    \n",
    "    def load_landmarks(self, video_path):\n",
    "        video_id = Path(video_path).stem\n",
    "        \n",
    "        if video_id in self.cached_landmarks:\n",
    "            landmarks = np.load(self.cached_landmarks[video_id])\n",
    "            if len(landmarks) < self.num_frames:\n",
    "                padding = np.tile(landmarks[-1:], (self.num_frames - len(landmarks), 1))\n",
    "                landmarks = np.vstack([landmarks, padding])\n",
    "            return landmarks[:self.num_frames]\n",
    "        \n",
    "        # Return zeros if no cached landmarks\n",
    "        return np.zeros((self.num_frames, 162), dtype=np.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Get frame trimming info if available\n",
    "        start_frame, end_frame = None, None\n",
    "        if self.frame_info is not None:\n",
    "            start_frame, end_frame = self.frame_info[idx]\n",
    "        \n",
    "        frames = self.load_video_frames(video_path, start_frame, end_frame)\n",
    "        \n",
    "        # Apply video augmentation if provided\n",
    "        if self.video_augment is not None:\n",
    "            frames = self.video_augment(frames)\n",
    "        \n",
    "        pixel_values = self.videomae_processor(\n",
    "            list(frames), \n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.squeeze(0)\n",
    "        \n",
    "        landmarks = self.load_landmarks(video_path)\n",
    "        \n",
    "        # Apply landmark augmentation if provided\n",
    "        if self.landmark_augment is not None:\n",
    "            landmarks = self.landmark_augment(landmarks)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'landmarks': torch.tensor(landmarks, dtype=torch.float32),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined (with frame trimming and augmentation support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88990e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAINER WITH CHECKPOINT SUPPORT AND REGULARIZATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class HybridASLTrainer:\n",
    "    \"\"\"Trainer with checkpoint resume support and regularization options.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device='cuda',\n",
    "                 learning_rate=1e-4, weight_decay=0.01, checkpoint_dir='checkpoints',\n",
    "                 label_smoothing=0.0):\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        videomae_params = list(model.visual_encoder.videomae.parameters())\n",
    "        other_params = [p for n, p in model.named_parameters() if 'videomae' not in n]\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': videomae_params, 'lr': learning_rate * 0.1},\n",
    "            {'params': other_params, 'lr': learning_rate}\n",
    "        ], weight_decay=weight_decay)\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=50, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Use label smoothing to reduce overfitting\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        self.start_epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        \n",
    "        if label_smoothing > 0:\n",
    "            print(f\"  Using label smoothing: {label_smoothing}\")\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_acc, filename=None):\n",
    "        if filename is None:\n",
    "            filename = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'val_acc': val_acc,\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = self.checkpoint_dir / filename\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.start_epoch = checkpoint['epoch'] + 1\n",
    "        self.best_val_acc = checkpoint['best_val_acc']\n",
    "        \n",
    "        print(f\"✓ Resumed from epoch {self.start_epoch}, best val acc: {self.best_val_acc:.2f}%\")\n",
    "        return self.start_epoch\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            pixel_values = batch['pixel_values'].to(self.device)\n",
    "            landmarks = batch['landmarks'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(pixel_values, landmarks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        correct_top5 = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in self.val_loader:\n",
    "            pixel_values = batch['pixel_values'].to(self.device)\n",
    "            landmarks = batch['landmarks'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            logits = self.model(pixel_values, landmarks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, top5_pred = logits.topk(5, dim=1)\n",
    "            correct_top5 += sum(labels[i] in top5_pred[i] for i in range(labels.size(0)))\n",
    "        \n",
    "        top1_acc = 100. * correct / total\n",
    "        top5_acc = 100. * correct_top5 / total\n",
    "        return total_loss / len(self.val_loader), top1_acc, top5_acc\n",
    "    \n",
    "    def train(self, num_epochs, save_path='best_hybrid_asl_model.pth', checkpoint_every=5):\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc, val_top5 = self.evaluate()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Top-1: {val_acc:.2f}%, Val Top-5: {val_top5:.2f}%\")\n",
    "            \n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "                print(f\"  ✓ New best model saved! ({val_acc:.2f}%)\")\n",
    "            \n",
    "            if (epoch + 1) % checkpoint_every == 0:\n",
    "                ckpt_path = self.save_checkpoint(epoch, val_acc)\n",
    "                print(f\"  ✓ Checkpoint saved: {ckpt_path}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        self.save_checkpoint(num_epochs - 1, val_acc, 'checkpoint_final.pth')\n",
    "        return self.best_val_acc\n",
    "\n",
    "print(\"✓ Trainer class defined (with label smoothing support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76709670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LOAD DATASET\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_wlasl_dataset(data_dir, subset_file='nslt_300.json'):\n",
    "    \"\"\"Load WLASL dataset with frame trimming info.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    videos_dir = data_dir / 'videos'\n",
    "    \n",
    "    # Load missing videos for faster filtering\n",
    "    missing_videos = set()\n",
    "    missing_path = data_dir / 'missing.txt'\n",
    "    if missing_path.exists():\n",
    "        with open(missing_path, 'r') as f:\n",
    "            missing_videos = {line.strip() for line in f if line.strip()}\n",
    "        print(f\"Loaded {len(missing_videos)} entries from missing.txt\")\n",
    "    \n",
    "    with open(data_dir / subset_file, 'r') as f:\n",
    "        subset_data = json.load(f)\n",
    "    \n",
    "    with open(data_dir / 'WLASL_v0.3.json', 'r') as f:\n",
    "        wlasl_data = json.load(f)\n",
    "    \n",
    "    video_id_to_gloss = {}\n",
    "    for entry in wlasl_data:\n",
    "        gloss = entry['gloss']\n",
    "        for instance in entry['instances']:\n",
    "            video_id_to_gloss[instance['video_id']] = gloss\n",
    "    \n",
    "    idx_to_gloss = {}\n",
    "    for video_id, info in subset_data.items():\n",
    "        class_idx = info['action'][0]\n",
    "        if video_id in video_id_to_gloss:\n",
    "            gloss = video_id_to_gloss[video_id]\n",
    "            if class_idx not in idx_to_gloss:\n",
    "                idx_to_gloss[class_idx] = gloss\n",
    "    \n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    frame_info = []  # (start_frame, end_frame) for each video\n",
    "    skipped = 0\n",
    "    \n",
    "    for video_id, info in subset_data.items():\n",
    "        # Skip if in missing.txt\n",
    "        if video_id in missing_videos:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        video_path = videos_dir / f\"{video_id}.mp4\"\n",
    "        if not video_path.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # action format: [class_idx, start_frame, end_frame]\n",
    "        video_paths.append(str(video_path))\n",
    "        labels.append(info['action'][0])\n",
    "        frame_info.append((info['action'][1], info['action'][2]))\n",
    "    \n",
    "    print(f\"Found {len(video_paths)} videos, {len(idx_to_gloss)} classes\")\n",
    "    if skipped > 0:\n",
    "        print(f\"Skipped {skipped} missing videos\")\n",
    "    print(f\"Frame trimming: ENABLED (using start_frame/end_frame from annotations)\")\n",
    "    \n",
    "    # Save label mapping\n",
    "    with open('label_mapping.json', 'w') as f:\n",
    "        json.dump({str(k): v for k, v in idx_to_gloss.items()}, f, indent=2)\n",
    "    \n",
    "    return video_paths, labels, idx_to_gloss, frame_info\n",
    "\n",
    "# Load dataset\n",
    "video_paths, labels, idx_to_gloss, frame_info = load_wlasl_dataset(\n",
    "    CONFIG['data_dir'], \n",
    "    CONFIG['subset']\n",
    ")\n",
    "num_classes = len(idx_to_gloss)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# CREATE DATALOADERS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "videomae_processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base')\n",
    "\n",
    "full_dataset = HybridASLDataset(\n",
    "    video_paths=video_paths,\n",
    "    labels=labels,\n",
    "    videomae_processor=videomae_processor,\n",
    "    num_frames=CONFIG['num_frames'],\n",
    "    landmarks_dir=CONFIG['landmarks_dir'],\n",
    "    frame_info=frame_info,  # Pass frame trimming info\n",
    "    video_augment=video_augment if CONFIG['augment'] else None,\n",
    "    landmark_augment=landmark_augment if CONFIG['augment'] else None\n",
    ")\n",
    "\n",
    "# 80/20 train/val split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca609df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# CREATE MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Creating model...\")\n",
    "\n",
    "# LoRA and freeze_videomae are mutually exclusive\n",
    "use_freeze = CONFIG['freeze_videomae'] and not CONFIG['use_lora']\n",
    "\n",
    "model = HybridASLModel(\n",
    "    num_classes=num_classes,\n",
    "    videomae_model='MCG-NJU/videomae-base',\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    freeze_videomae=use_freeze,\n",
    "    fusion_type=CONFIG['fusion_type'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    use_lora=CONFIG['use_lora'],\n",
    "    lora_rank=CONFIG['lora_rank'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout']\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✓ Total parameters: {total_params:,}\")\n",
    "print(f\"✓ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"✓ Frozen VideoMAE: {use_freeze}\")\n",
    "print(f\"✓ LoRA: {'ENABLED (rank=' + str(CONFIG['lora_rank']) + ')' if CONFIG['use_lora'] else 'disabled'}\")\n",
    "print(f\"✓ Dropout: {CONFIG['dropout']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2906a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAIN WITH CHECKPOINT RESUME\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "trainer = HybridASLTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=CONFIG['device'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],  # Use weight decay from config (0.05)\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir'],\n",
    "    label_smoothing=CONFIG['label_smoothing']  # Use label smoothing from config (0.1)\n",
    ")\n",
    "\n",
    "# Check for existing checkpoint to resume from\n",
    "checkpoint_path = Path(CONFIG['checkpoint_dir']) / 'checkpoint_final.pth'\n",
    "if not checkpoint_path.exists():\n",
    "    # Try to find the latest periodic checkpoint\n",
    "    checkpoints = list(Path(CONFIG['checkpoint_dir']).glob('checkpoint_epoch_*.pth'))\n",
    "    if checkpoints:\n",
    "        # Sort by epoch number and get the latest\n",
    "        checkpoint_path = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "    trainer.load_checkpoint(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_acc = trainer.train(\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    save_path='/kaggle/working/best_hybrid_asl_model.pth',\n",
    "    checkpoint_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAINING COMPLETE\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best validation Top-1 accuracy: {best_acc:.2f}%\")\n",
    "print(f\"Model saved to: /kaggle/working/best_hybrid_asl_model.pth\")\n",
    "print(f\"Checkpoints saved to: {CONFIG['checkpoint_dir']}/\")\n",
    "print(f\"Label mapping saved to: label_mapping.json\")\n",
    "\n",
    "# List output files\n",
    "print(\"\\nOutput files:\")\n",
    "for f in Path('/kaggle/working').glob('*'):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
