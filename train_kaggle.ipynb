{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a184b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers>=4.30.0 mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to /kaggle/working\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': '/kaggle/input/wlalsl-with-landmarks/data/wlasl',  # Changed\n",
    "    'landmarks_dir': '/kaggle/input/wlalsl-with-landmarks/data/wlasl/landmarks',  # Changed\n",
    "    'checkpoint_dir': '/kaggle/working/checkpoints',\n",
    "    'subset': 'nslt_300.json',\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 50,\n",
    "    'num_frames': 16,\n",
    "    'hidden_dim': 256,\n",
    "    'learning_rate': 1e-4,\n",
    "    'fusion_type': 'concat',\n",
    "    'device': 'cuda',\n",
    "}\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MediaPipe models\n",
    "import urllib.request\n",
    "\n",
    "def download_mediapipe_models():\n",
    "    \"\"\"Download MediaPipe task models to working directory.\"\"\"\n",
    "    models = {\n",
    "        'hand_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',\n",
    "        'pose_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task'\n",
    "    }\n",
    "    \n",
    "    for filename, url in models.items():\n",
    "        if not Path(filename).exists():\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            print(f\"✓ Downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"✓ {filename} already exists\")\n",
    "\n",
    "download_mediapipe_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy source files from input dataset if using custom dataset structure\n",
    "# If your dataset includes the python files, copy them:\n",
    "source_files = ['hybrid_asl_model.py', 'train_hybrid_asl.py']\n",
    "for f in source_files:\n",
    "    src = Path(f'/kaggle/input/wlasl/{f}')\n",
    "    dst = Path(f'/kaggle/working/{f}')\n",
    "    if src.exists() and not dst.exists():\n",
    "        import shutil\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"Copied {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d483c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import VideoMAEModel, VideoMAEImageProcessor\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# MODEL DEFINITIONS (inline for Kaggle notebook portability)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class LandmarkEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder for landmark sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=162, hidden_dim=256, num_heads=4, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 64, hidden_dim) * 0.02)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VideoMAEEncoder(nn.Module):\n",
    "    \"\"\"VideoMAE encoder for visual features.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='MCG-NJU/videomae-base', hidden_dim=256, freeze_backbone=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.videomae = VideoMAEModel.from_pretrained(model_name)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.videomae.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        videomae_hidden = self.videomae.config.hidden_size\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(videomae_hidden, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.videomae(pixel_values=pixel_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        features = self.projection(pooled)\n",
    "        return features\n",
    "\n",
    "\n",
    "class HybridASLModel(nn.Module):\n",
    "    \"\"\"Hybrid model combining VideoMAE and MediaPipe landmarks.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, videomae_model='MCG-NJU/videomae-base', hidden_dim=256,\n",
    "                 freeze_videomae=False, dropout=0.3, fusion_type='concat'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.visual_encoder = VideoMAEEncoder(\n",
    "            model_name=videomae_model,\n",
    "            hidden_dim=hidden_dim,\n",
    "            freeze_backbone=freeze_videomae,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.landmark_encoder = LandmarkEncoder(\n",
    "            input_dim=162,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        if fusion_type == 'concat':\n",
    "            fusion_input_dim = hidden_dim * 2\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(fusion_input_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        elif fusion_type == 'attention':\n",
    "            self.fusion_attention = nn.MultiheadAttention(\n",
    "                embed_dim=hidden_dim,\n",
    "                num_heads=4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        elif fusion_type == 'gated':\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, pixel_values, landmarks):\n",
    "        visual_features = self.visual_encoder(pixel_values)\n",
    "        landmark_features = self.landmark_encoder(landmarks)\n",
    "        \n",
    "        if self.fusion_type == 'concat':\n",
    "            fused = torch.cat([visual_features, landmark_features], dim=-1)\n",
    "            fused = self.fusion(fused)\n",
    "        elif self.fusion_type == 'attention':\n",
    "            v_expanded = visual_features.unsqueeze(1)\n",
    "            l_expanded = landmark_features.unsqueeze(1)\n",
    "            attended_v, _ = self.fusion_attention(v_expanded, l_expanded, l_expanded)\n",
    "            attended_l, _ = self.fusion_attention(l_expanded, v_expanded, v_expanded)\n",
    "            fused = torch.cat([attended_v.squeeze(1), attended_l.squeeze(1)], dim=-1)\n",
    "            fused = self.fusion(fused)\n",
    "        elif self.fusion_type == 'gated':\n",
    "            combined = torch.cat([visual_features, landmark_features], dim=-1)\n",
    "            gate = self.gate(combined)\n",
    "            fused = gate * visual_features + (1 - gate) * landmark_features\n",
    "            fused = self.fusion(fused)\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, pixel_values, landmarks):\n",
    "        logits = self.forward(pixel_values, landmarks)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        confidence, predictions = probs.max(dim=-1)\n",
    "        return predictions, confidence, probs\n",
    "\n",
    "print(\"✓ Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ccb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# DATASET CLASS WITH LANDMARK CACHING AND FRAME TRIMMING\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class HybridASLDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with pre-extracted landmark support and frame trimming.\"\"\"\n",
    "    \n",
    "    def __init__(self, video_paths, labels, videomae_processor, num_frames=16, \n",
    "                 image_size=224, landmarks_dir=None, frame_info=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.videomae_processor = videomae_processor\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.landmarks_dir = Path(landmarks_dir) if landmarks_dir else None\n",
    "        self.frame_info = frame_info  # (start_frame, end_frame) for each video\n",
    "        \n",
    "        # Build landmarks cache lookup\n",
    "        self.cached_landmarks = {}\n",
    "        if self.landmarks_dir:\n",
    "            for vp in video_paths:\n",
    "                video_id = Path(vp).stem\n",
    "                landmark_path = self.landmarks_dir / f\"{video_id}_landmarks.npy\"\n",
    "                if landmark_path.exists():\n",
    "                    self.cached_landmarks[video_id] = str(landmark_path)\n",
    "            print(f\"  Found {len(self.cached_landmarks)}/{len(video_paths)} pre-extracted landmarks\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def load_video_frames(self, video_path, start_frame=None, end_frame=None):\n",
    "        \"\"\"Load video frames with optional frame trimming.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Apply frame trimming if specified (convert 1-indexed to 0-indexed)\n",
    "        if start_frame is not None and end_frame is not None:\n",
    "            frame_start = max(0, start_frame - 1)\n",
    "            frame_end = min(total_frames - 1, end_frame - 1)\n",
    "            segment_length = frame_end - frame_start + 1\n",
    "        else:\n",
    "            frame_start = 0\n",
    "            frame_end = total_frames - 1\n",
    "            segment_length = total_frames\n",
    "        \n",
    "        # Sample frames uniformly within the trimmed segment\n",
    "        if segment_length <= self.num_frames:\n",
    "            indices = [frame_start + i for i in range(segment_length)]\n",
    "        else:\n",
    "            indices = np.linspace(frame_start, frame_end, self.num_frames, dtype=int).tolist()\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1] if frames else np.zeros((self.image_size, self.image_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        return frames[:self.num_frames]\n",
    "    \n",
    "    def load_landmarks(self, video_path):\n",
    "        video_id = Path(video_path).stem\n",
    "        \n",
    "        if video_id in self.cached_landmarks:\n",
    "            landmarks = np.load(self.cached_landmarks[video_id])\n",
    "            if len(landmarks) < self.num_frames:\n",
    "                padding = np.tile(landmarks[-1:], (self.num_frames - len(landmarks), 1))\n",
    "                landmarks = np.vstack([landmarks, padding])\n",
    "            return landmarks[:self.num_frames]\n",
    "        \n",
    "        # Return zeros if no cached landmarks\n",
    "        return np.zeros((self.num_frames, 162), dtype=np.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Get frame trimming info if available\n",
    "        start_frame, end_frame = None, None\n",
    "        if self.frame_info is not None:\n",
    "            start_frame, end_frame = self.frame_info[idx]\n",
    "        \n",
    "        frames = self.load_video_frames(video_path, start_frame, end_frame)\n",
    "        pixel_values = self.videomae_processor(\n",
    "            list(frames), \n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.squeeze(0)\n",
    "        \n",
    "        landmarks = self.load_landmarks(video_path)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'landmarks': torch.tensor(landmarks, dtype=torch.float32),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined (with frame trimming support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88990e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAINER WITH CHECKPOINT SUPPORT\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class HybridASLTrainer:\n",
    "    \"\"\"Trainer with checkpoint resume support.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device='cuda',\n",
    "                 learning_rate=1e-4, weight_decay=0.01, checkpoint_dir='checkpoints'):\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        videomae_params = list(model.visual_encoder.videomae.parameters())\n",
    "        other_params = [p for n, p in model.named_parameters() if 'videomae' not in n]\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': videomae_params, 'lr': learning_rate * 0.1},\n",
    "            {'params': other_params, 'lr': learning_rate}\n",
    "        ], weight_decay=weight_decay)\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=50, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.start_epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_acc, filename=None):\n",
    "        if filename is None:\n",
    "            filename = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'val_acc': val_acc,\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = self.checkpoint_dir / filename\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.start_epoch = checkpoint['epoch'] + 1\n",
    "        self.best_val_acc = checkpoint['best_val_acc']\n",
    "        \n",
    "        print(f\"✓ Resumed from epoch {self.start_epoch}, best val acc: {self.best_val_acc:.2f}%\")\n",
    "        return self.start_epoch\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            pixel_values = batch['pixel_values'].to(self.device)\n",
    "            landmarks = batch['landmarks'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(pixel_values, landmarks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        correct_top5 = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in self.val_loader:\n",
    "            pixel_values = batch['pixel_values'].to(self.device)\n",
    "            landmarks = batch['landmarks'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            logits = self.model(pixel_values, landmarks)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, top5_pred = logits.topk(5, dim=1)\n",
    "            correct_top5 += sum(labels[i] in top5_pred[i] for i in range(labels.size(0)))\n",
    "        \n",
    "        top1_acc = 100. * correct / total\n",
    "        top5_acc = 100. * correct_top5 / total\n",
    "        return total_loss / len(self.val_loader), top1_acc, top5_acc\n",
    "    \n",
    "    def train(self, num_epochs, save_path='best_hybrid_asl_model.pth', checkpoint_every=5):\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc, val_top5 = self.evaluate()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Top-1: {val_acc:.2f}%, Val Top-5: {val_top5:.2f}%\")\n",
    "            \n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "                print(f\"  ✓ New best model saved! ({val_acc:.2f}%)\")\n",
    "            \n",
    "            if (epoch + 1) % checkpoint_every == 0:\n",
    "                ckpt_path = self.save_checkpoint(epoch, val_acc)\n",
    "                print(f\"  ✓ Checkpoint saved: {ckpt_path}\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        self.save_checkpoint(num_epochs - 1, val_acc, 'checkpoint_final.pth')\n",
    "        return self.best_val_acc\n",
    "\n",
    "print(\"✓ Trainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76709670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# LOAD DATASET\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_wlasl_dataset(data_dir, subset_file='nslt_300.json'):\n",
    "    \"\"\"Load WLASL dataset with frame trimming info.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    videos_dir = data_dir / 'videos'\n",
    "    \n",
    "    # Load missing videos for faster filtering\n",
    "    missing_videos = set()\n",
    "    missing_path = data_dir / 'missing.txt'\n",
    "    if missing_path.exists():\n",
    "        with open(missing_path, 'r') as f:\n",
    "            missing_videos = {line.strip() for line in f if line.strip()}\n",
    "        print(f\"Loaded {len(missing_videos)} entries from missing.txt\")\n",
    "    \n",
    "    with open(data_dir / subset_file, 'r') as f:\n",
    "        subset_data = json.load(f)\n",
    "    \n",
    "    with open(data_dir / 'WLASL_v0.3.json', 'r') as f:\n",
    "        wlasl_data = json.load(f)\n",
    "    \n",
    "    video_id_to_gloss = {}\n",
    "    for entry in wlasl_data:\n",
    "        gloss = entry['gloss']\n",
    "        for instance in entry['instances']:\n",
    "            video_id_to_gloss[instance['video_id']] = gloss\n",
    "    \n",
    "    idx_to_gloss = {}\n",
    "    for video_id, info in subset_data.items():\n",
    "        class_idx = info['action'][0]\n",
    "        if video_id in video_id_to_gloss:\n",
    "            gloss = video_id_to_gloss[video_id]\n",
    "            if class_idx not in idx_to_gloss:\n",
    "                idx_to_gloss[class_idx] = gloss\n",
    "    \n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    frame_info = []  # (start_frame, end_frame) for each video\n",
    "    skipped = 0\n",
    "    \n",
    "    for video_id, info in subset_data.items():\n",
    "        # Skip if in missing.txt\n",
    "        if video_id in missing_videos:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        video_path = videos_dir / f\"{video_id}.mp4\"\n",
    "        if not video_path.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # action format: [class_idx, start_frame, end_frame]\n",
    "        video_paths.append(str(video_path))\n",
    "        labels.append(info['action'][0])\n",
    "        frame_info.append((info['action'][1], info['action'][2]))\n",
    "    \n",
    "    print(f\"Found {len(video_paths)} videos, {len(idx_to_gloss)} classes\")\n",
    "    if skipped > 0:\n",
    "        print(f\"Skipped {skipped} missing videos\")\n",
    "    print(f\"Frame trimming: ENABLED (using start_frame/end_frame from annotations)\")\n",
    "    \n",
    "    # Save label mapping\n",
    "    with open('label_mapping.json', 'w') as f:\n",
    "        json.dump({str(k): v for k, v in idx_to_gloss.items()}, f, indent=2)\n",
    "    \n",
    "    return video_paths, labels, idx_to_gloss, frame_info\n",
    "\n",
    "# Load dataset\n",
    "video_paths, labels, idx_to_gloss, frame_info = load_wlasl_dataset(\n",
    "    CONFIG['data_dir'], \n",
    "    CONFIG['subset']\n",
    ")\n",
    "num_classes = len(idx_to_gloss)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# CREATE DATALOADERS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "videomae_processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base')\n",
    "\n",
    "full_dataset = HybridASLDataset(\n",
    "    video_paths=video_paths,\n",
    "    labels=labels,\n",
    "    videomae_processor=videomae_processor,\n",
    "    num_frames=CONFIG['num_frames'],\n",
    "    landmarks_dir=CONFIG['landmarks_dir'],\n",
    "    frame_info=frame_info  # Pass frame trimming info\n",
    ")\n",
    "\n",
    "# 80/20 train/val split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca609df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# CREATE MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Creating model...\")\n",
    "\n",
    "model = HybridASLModel(\n",
    "    num_classes=num_classes,\n",
    "    videomae_model='MCG-NJU/videomae-base',\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    freeze_videomae=False,\n",
    "    fusion_type=CONFIG['fusion_type'],\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✓ Total parameters: {total_params:,}\")\n",
    "print(f\"✓ Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2906a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAIN WITH CHECKPOINT RESUME\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "trainer = HybridASLTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=CONFIG['device'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir']\n",
    ")\n",
    "\n",
    "# Check for existing checkpoint to resume from\n",
    "checkpoint_path = Path(CONFIG['checkpoint_dir']) / 'checkpoint_final.pth'\n",
    "if not checkpoint_path.exists():\n",
    "    # Try to find the latest periodic checkpoint\n",
    "    checkpoints = list(Path(CONFIG['checkpoint_dir']).glob('checkpoint_epoch_*.pth'))\n",
    "    if checkpoints:\n",
    "        # Sort by epoch number and get the latest\n",
    "        checkpoint_path = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"Found existing checkpoint: {checkpoint_path}\")\n",
    "    trainer.load_checkpoint(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_acc = trainer.train(\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    save_path='/kaggle/working/best_hybrid_asl_model.pth',\n",
    "    checkpoint_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TRAINING COMPLETE\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best validation Top-1 accuracy: {best_acc:.2f}%\")\n",
    "print(f\"Model saved to: /kaggle/working/best_hybrid_asl_model.pth\")\n",
    "print(f\"Checkpoints saved to: {CONFIG['checkpoint_dir']}/\")\n",
    "print(f\"Label mapping saved to: label_mapping.json\")\n",
    "\n",
    "# List output files\n",
    "print(\"\\nOutput files:\")\n",
    "for f in Path('/kaggle/working').glob('*'):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
