# Hybrid ASL Recognition System

A hybrid American Sign Language (ASL) recognition system combining deep visual features with explicit hand/pose landmarks for improved accuracy.

## Overview

This project implements a research-oriented ASL recognition system that fuses:
- **Visual Features**: Deep learning models (VideoMAE or Gemma 3) for visual understanding
- **MediaPipe Landmarks**: Explicit hand and pose keypoints for geometric robustness

## Features

âœ¨ **Dual Architecture Support:**
- **VideoMAE-based** (original): Full end-to-end training with visual transformer
- **Gemma 3-based** (new): 10-50x faster training with frozen feature extraction

âœ¨ **Multiple Fusion Strategies:**
- Concatenation fusion
- Cross-attention fusion
- Gated fusion

âœ¨ **Production-Ready:**
- Real-time webcam inference
- Pre-extraction pipelines for fast iteration
- Data augmentation for better generalization
- Checkpoint support and resumable training

## Quick Start

### Installation

```bash
# Install base requirements
pip install -r requirements.txt

# For Gemma 3 support (optional)
pip install -r requirements_gemma.txt
```

### Training (VideoMAE-based)

```bash
# Train with VideoMAE (original method)
python train_hybrid_asl.py \
    --data_dir data/wlasl \
    --subset nslt_100.json \
    --batch_size 8 \
    --epochs 50
```

### Training (Gemma 3-based - Faster!)

See [GEMMA_INTEGRATION.md](GEMMA_INTEGRATION.md) for complete guide.

```bash
# Step 1: Pre-extract landmarks
python preprocess_landmarks.py \
    --data_dir data/wlasl \
    --output_dir data/wlasl/landmarks

# Step 2: Pre-extract Gemma features
python preprocess_gemma_features.py \
    --data_dir data/wlasl \
    --output_dir data/wlasl/gemma_features

# Step 3: Train (fast!)
python train_simple.py \
    --data_dir data/wlasl \
    --gemma_features_dir data/wlasl/gemma_features \
    --landmarks_dir data/wlasl/landmarks
```

### Real-time Webcam Inference

```bash
python webcam_hybrid_asl.py --device cuda
```

## Architecture

### Original (VideoMAE-based)

```
Video â†’ [VideoMAE Branch] â†’ Visual Features (256-dim) â”€â”
                                                       â”œâ”€â†’ Fusion â†’ Classifier
Video â†’ [MediaPipe Branch] â†’ Landmark Features (256-dim)â”˜
```

**Training:** ~30-60 min/epoch, ~88M trainable params, 12-16 GB GPU memory

### New (Gemma 3-based)

```
Pre-extraction (run once):
    Video â†’ [Gemma 3 Vision Encoder (FROZEN)] â†’ Save .npy features

Training (fast):
    Gemma Features â†’ [Visual Projection] â”€â”
                                          â”œâ”€â†’ Fusion â†’ Classifier
    Landmarks â†’ [Landmark Encoder] â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training:** ~5-10 min/epoch, ~2.8M trainable params, 2-4 GB GPU memory

**Speedup:** 10-50x faster development cycle! ğŸš€

## Project Structure

```
.
â”œâ”€â”€ hybrid_asl_model.py              # Original VideoMAE-based model
â”œâ”€â”€ hybrid_asl_model_simple.py       # New Gemma-based simplified model
â”œâ”€â”€ train_hybrid_asl.py              # Training script for VideoMAE
â”œâ”€â”€ train_simple.py                  # Training script for Gemma
â”œâ”€â”€ gemma_feature_extractor.py       # Gemma feature extraction
â”œâ”€â”€ preprocess_landmarks.py          # Landmark pre-extraction
â”œâ”€â”€ preprocess_gemma_features.py     # Gemma feature pre-extraction
â”œâ”€â”€ webcam_hybrid_asl.py            # Real-time inference
â”œâ”€â”€ augmentations.py                 # Data augmentation utilities
â”œâ”€â”€ evaluate_model.py                # Model evaluation
â”œâ”€â”€ requirements.txt                 # Base dependencies
â”œâ”€â”€ requirements_gemma.txt           # Gemma-specific dependencies
â”œâ”€â”€ GEMMA_INTEGRATION.md            # Detailed Gemma guide
â””â”€â”€ test_gemma_integration.py       # Test suite
```

## Performance Comparison

| Metric | VideoMAE | Gemma 3 |
|--------|----------|---------|
| Trainable Params | ~88M | ~2.8M |
| GPU Memory (Training) | 12-16 GB | 2-4 GB |
| Training Time/Epoch | 30-60 min | 5-10 min |
| Max Batch Size | 8-16 | 32-64 |
| Pre-extraction Time | N/A | 30-60 min (one-time) |
| Total Training (50 epochs) | 25-50 hours | ~30 min + 4-8 hours |

## Dataset

This project uses the [WLASL dataset](https://dxli94.github.io/WLASL/) (Word-Level American Sign Language):
- 2,000+ signs
- 21,000+ videos
- Subset configurations: 100, 300, 1000, 2000 classes

### Data Structure

```
data/wlasl/
â”œâ”€â”€ videos/              # Video files: {video_id}.mp4
â”œâ”€â”€ WLASL_v0.3.json     # Main annotations
â”œâ”€â”€ nslt_100.json       # 100-class subset
â”œâ”€â”€ nslt_300.json       # 300-class subset
â”œâ”€â”€ nslt_1000.json      # 1000-class subset
â”œâ”€â”€ nslt_2000.json      # 2000-class subset
â”œâ”€â”€ missing.txt         # List of unavailable videos
â”œâ”€â”€ landmarks/          # Pre-extracted landmarks (optional)
â””â”€â”€ gemma_features/     # Pre-extracted Gemma features (optional)
```

## MediaPipe Landmark Features (162 dims/frame)

| Component | Indices | Size | Description |
|-----------|---------|------|-------------|
| Left hand | 0-62 | 63 | 21 landmarks Ã— 3 coords (x,y,z) |
| Right hand | 63-125 | 63 | 21 landmarks Ã— 3 coords |
| Upper body | 126-161 | 36 | 12 pose landmarks (shoulders, elbows, wrists, hips) |

## Training Tips

### For VideoMAE-based Training:
- Use batch size 4-8 on standard GPUs
- Enable `--freeze_videomae` for faster training
- Use `--augment` to reduce overfitting
- Pre-extract landmarks with `preprocess_landmarks.py` for 10x speedup

### For Gemma 3-based Training:
- Pre-extract both landmarks AND Gemma features first
- Can use batch size 32-64
- Much faster iteration for hyperparameter tuning
- SignGemma-ready: just re-run feature extraction when available

## Advanced Usage

### Data Augmentation

```bash
python train_hybrid_asl.py \
    --augment \
    --augment_strength medium \
    ...
```

### Fusion Strategy Ablation

```bash
# Test different fusion strategies
for fusion in concat attention gated; do
    python train_simple.py --fusion_type $fusion ...
done
```

### Resume from Checkpoint

```bash
python train_hybrid_asl.py \
    --resume checkpoints/checkpoint_epoch_25.pth \
    ...
```

## Testing

Run the test suite to verify the installation:

```bash
# Test Gemma integration
python test_gemma_integration.py
```

Expected output: `5/5 tests passed` âœ…

## SignGemma Support

When Google releases SignGemma (trained on 10,000+ hours of ASL), you can easily swap to it:

```bash
# Re-extract features with SignGemma
python preprocess_gemma_features.py \
    --model_name google/signgemma-MODEL_NAME \
    --output_dir data/wlasl/signgemma_features \
    ...

# Train with SignGemma features
python train_simple.py \
    --gemma_features_dir data/wlasl/signgemma_features \
    ...
```

No code changes needed! ğŸ¯

## Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{li2020wlasl,
  title={Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison},
  author={Li, Dongxu and others},
  booktitle={WACV},
  year={2020}
}

@inproceedings{tong2022videomae,
  title={VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author={Tong, Zhan and others},
  booktitle={NeurIPS},
  year={2022}
}
```

## References

- [WLASL Dataset](https://dxli94.github.io/WLASL/)
- [VideoMAE](https://github.com/MCG-NJU/VideoMAE)
- [Gemma 3](https://huggingface.co/google/gemma-3-4b-it)
- [SignGemma Announcement](https://multilingual.com/google-signgemma-on-device-asl-translation/)
- [MediaPipe](https://developers.google.com/mediapipe)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

See LICENSE file for details.

## Acknowledgements

- WLASL dataset creators
- Google Research for Gemma and MediaPipe
- VideoMAE authors
- Open source community

---

**Note:** The Gemma 3 integration is production-ready. For detailed usage, see [GEMMA_INTEGRATION.md](GEMMA_INTEGRATION.md).
